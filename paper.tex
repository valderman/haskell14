%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\input{macros}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{ICFP '14}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Type-Safe Client-Server Communication in Web Applications}
\subtitle{Subtitle Text, if any}

\authorinfo{Koen Claessen and Anton Ekblad}
           {Chalmers University of Technology}
           {\{koen,antonek\}@chalmers.se}

\maketitle

\begin{abstract}
This is the text of the abstract.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
%\terms
%term1, term2
%
\keywords
web

\section{Introduction}

Development of web applications is no task for the faint of heart.
The conventional method involves splitting your program into two logical parts,
writing the one in Javascript, which is notorious even among its proponents for
being wonky and error-prone, and the other in the current language du jour.
Then, two are glued together using whichever home-grown network protocol seems
to fit the application. However, most web applications are conceptually
monolithic entities, making this forced split an undesirable hindrance which
introduces new possibilities for defects, adds development overhead and
prevents code reuse.

Several attempts have been made at tackling this problem, but none is, in our
view, satisfactory. In this paper, we propose a functional programming model in
which a web application is written as a single program from which client and
server binaries are generated during compilation. Type annotations in the source
program control which parts are executed on the server and which are executed
on the client, and the two communicate using type safe RPC calls. Functions
which are not explicitly declared as server-side or client-side are usable by
either side. Our solution is implemented as a Haskell library requiring minimal
to no compiler support, ensuring portability and modularity.

\paragraph{Motivation}

Code written in Javascript, the only widely supported language for client-side
web applications, is often confusing and error-prone, much due to the
language's lack of modularity, encapsulation facilities and type safety.

Worse, most web applications, being intended to facilitate communication, data
storage and other tasks involving some centralized resource, also require a
significant server component. This component is usually implemented as a
completely separate program, and communicates with the client code over some
network protocol.

This state of things is not a conscious design choice - most web applications
are conceptually a single entity, not two programs which just happen to talk
to each other over a network - but a consequence of there being a large,
distributed network between the client and server parts.
However, such implementation details should not be allowed to dictate the way
we structure and reason about our applications - clearly, an abstraction is
called for.

To put make the problem a bit more concrete, let's take a look at an example.
Let's say what we want to implement a simple ``chatbox'' component for a
website, to allow visitors to discuss the site's content in real time. Using
mainstream development practives and recent technologies such as WebSockets,
we may come up with something like the program in figure
\ref{lst:javascript-client}.

\begin{listingfloat}
\begin{code}
function handshake(sock) {sock.send('helo');}
function chat(sock, msg) {sock.send('text' + msg);}

window.onload = function() {
  var logbox = document.getElementById('log');
  var msgbox = document.getElementById('message');
  var sock = new WebSocket('ws://example.com);

  sock.onmessage = function(e) {
    logbox.value = e.data + NEWLINE + logbox.value;
  };

  sock.onopen = function(e) {
    handshake(sock);
    msgbox.addEventListener('keydown', function(e) {
      if(e.keyCode == 13) {
        var msg = msgbox.value;
        msgbox.value = '';
        chat(msg);
      }
    });
  };
};
\end{code}
\caption{Javascript chatbox implementation}
\label{lst:javascript-client}
\end{listingfloat}

Since the ``chatbox'' application is very simple - users should only be able to
send and receive text messages in real time - we opt for a very simple
design. Two UI elements, \lstinline!logbox! and \lstinline!msgbox!, represent
the chat log and the text area where the user inputs their messages
respectively. When a message arrives, it is prepended to the chat log, making
the most recent message appear at the of the log window, and when the user hits
the return key in the input text box the message contained therein is sent and
the input text box is cleared.

Messages are transmitted as strings, with the initial four characters
indicating the type of the message and the rest being the optional payload.
There are only two messages; a handshake indicating that a user wants to join
the conversation, and a broadcast message which sends a line of text to all
connected users via the server. The only messages received from the server are
new chat messages, delivered as simple strings.

This code looks solid enough by web standards, but even this simple piece of
code contains no less than three asynchronous callbacks, two of which both read
and and modify the application's global state. This makes the program flow
non-obvious, and introduces unnecessary risk and complexity through the
haphazard state modifications.

Moreover, this code is not very extensible. If this simple application is to be
enhanced with new features down the road, the network protocol will clearly
need to be redesigned. However, if we were developing this application for a
client, said client would likely not want to pay the added cost for the design
and implementation of features she did not - and perhaps never will - ask for.

Should the protocol need updating in the future, how much time will we need to
spend on ensuring that the protocol is used properly across our entire program,
and how much extra work will it take to keep the client and server in sync?
How much code will need to be written twice, once for the client and once for
the server, due to the unfortunate fact that the two parts are implemented as
separate programs, possibly in separate languages?

Above all, is it really necessary for such a simple program to involve
client/server architectures and network protocol design at all?

\section{A seamless programming model}

There are many conveivable improvements to the mainstream web development model
described in the previous section. We propose an alternative programming model
based on Haskell, in which web applications are written as monolithic entities
rather than as two independent parts that just so happen to talk to each other.

Our proposed model, dubbed ``Haste.App'' in an ingenious attempt at
capitalizing on the recent popularity of the ``app'' buzzword, has the
following properties:

\begin{itemize}
  \item The programming model is synchronous, giving the programmer a simple,
        linear view of the program flow, eliminating the need to program with
        callbacks and continuations.
  \item Side-effecting code is explicitly designated to run on either the
        client or the server using the type system while pure code can be
        shared by both. Additionally, both client and server code may lift
        general IO computations, allowing for safe IO code reuse within the
        confines of the client or server designated code sections.
  \item Client-server network communication is handled through statically typed
        RPC function calls, extending the reach of Haskell's type checker over
        the network and giving the programmer advance warning when she uses
        network services incorrectly or forgets to update communication code
        as the application's internal protocol changes.
  \item Our model takes the view that the client side is the main driver when
        developing web applications and accordingly delegates the server to
        the role of a computational and/or storage resource, tasked with
        servicing client requests rather than driving the program. While it is
        entirely possible to implement a server-to-client communication channel
        on top of our model, we believe that choosing one side of the
        heterogenous client-server relation as the master helps keeping the
        program flow linear and predictable.
  \item The implementation is built as a library on top of the GHC and Haste
        Haskell compilers, requiring little to no compiler support. Programs
        are compiled twice; once with Haste and once with GHC, to produce the
        final client and server side code respectively.
\end{itemize}

\subsection{A first example}\label{sec:helloserver}

While explaining of the properties of our solution is all well and good,
nothing compares to a good old Hello World example to convey the idea.
We begin by implementing a function which prints a greeting to the server's
console.

\begin{code}
import Haste.App

helloServer :: String -> Server ()
helloServer name =
  liftIO $ putStrLn (name ++ " says hello!")
\end{code}

Computations exclusive to the server side live in the \lstinline!Server! monad.
This is basically an IO monad, as can be seen from the regular
\lstinline!putStrLn! \lstinline!IO! computation being lifted into it, with a
few extra operations for session handling; its main purpose is to prevent the
programmer from accidentally attempting to perform client-exclusive operations,
such as popping up a browser dialog box, on the server.

Next, we need to make the \lstinline!helloServer! function available as an RPC
function and call it from the client.

\begin{code}
appMain :: App Done
appMain = do
  greetings <- export helloServer

  runClient $ do
    name <- prompt "Hi there, what is your name?"
    onServer (greetings <.> name)
\end{code}

The \lstinline!export! function takes an arbitrary function, provided that all
its arguments as well as its return value are serializable through the aptly
named \lstinline!Serializable! type class, and returns a typed ``handle'' of
sorts. In this example, the type of \lstinline!greetings! is
\lstinline!Export (String -> Server ())!, indicating that the handle identifies
an exported function with a single \lstinline!String! argument and no return
value; exported functions all live in the \lstinline!Server! monad.

After the \lstinline!export! call, we enter the domain of client-exclusive code
with the application of \lstinline!runClient!. This function, unsurprisingly,
execute computations in the \lstinline!Client! monad which is essentially an
IO monad with cooperative multitasking added on top, to mitigate the fact that
Javascript has no native concurrency support. \lstinline!runClient! does not
return, and is the only function with a return type of \lstinline!App Done!,
which ensures that each \lstinline!App! computation contains exactly one client
computation.

In order to make an RPC call using a handle obtained from \lstinline!export!,
we must supply it with an argument. This is done using the \lstinline!<.>!
operator. Upon closer inspection, the attentive reader may notice that
\linebreak
its type, \lstinline!Serialize a => Export (a -> b) -> a -> Export b!,
is very similar to the type of the \lstinline!<*>! operator over applicative
functors. This is not a coincidence; the \lstinline!<.>! performs the same role
for the \lstinline!Export! type as \lstinline!<*>! performs for applicative
functors. The reason for using a separate operator for this instead of making
\lstinline!Export! an instance of \lstinline!Applicative! is that since
functions embedded in the \lstinline!Export! type exist only to be called over
a network, such functions must only be applied to arguments which can be
serialized and sent over a network connection. When an \lstinline!Export!
function is applied to an argument using \lstinline!<.>!, the argument is
serialized and stored inside the resulting \lstinline!Export! object, awaiting
dispatch.

After applying the value obtained by \lstinline!prompt!ing the user for her
name to the exported function, we apply the \lstinline!onServer! function to
the result, which dispatches the RPC call to the server. \lstinline!onServer!
will then block until the RPC call returns.

Finally, we need to launch our application, and inform the client part of the
whereabouts of its server counterpart; in this example the server will listen
on port 1111, and since we will only run this application locally the client
should attempt to contact the server at our local machine.

\begin{code}
main :: IO ()
main =
  runApp (mkConfig "ws://localhost:1111" 1111) appMain
\end{code}

This part is entirely straightforward; the URL of the server is provided for
the benefit of the client and the port to listen on is provided for the
benefit of the server. The final argument is the main function of our
application. Figure \ref{lst:hello-server} gives the code of this example in
its entirety.

Like \lstinline!runClient!, \lstinline!runApp! does not return, and is intended
to be the only computation executed by \lstinline!main!. In fact, performing
non-deterministic IO and allowing \lstinline!runApp! or its arguments to
capture the result may lead to unpredictable behavior. The \lstinline!appMain!
function should for all intents and purposes be regarded as the program's entry
point; a non-intrusive method of enforcing this using the type system is
described in section \ref{sec:limitations}, but is not used in the examples
given here as it relies on the use of the GHC plugin system.

\begin{listingfloat}
\begin{code}
import Haste.App

helloServer :: String -> Server ()
helloServer name =
  liftIO $ putStrLn (name ++ " says hello!")

appMain :: App Done
appMain = do
  greetings <- export helloServer

  runClient $ do
    name <- prompt "Hi there, what is your name?"
    onServer (greetings <.> name)

main :: IO ()
main =
  runApp (mkConfig "ws://localhost:1111" 1111) appMain
\end{code}
\caption{A seamless programming model: Hello Server}
\label{lst:hello-server}
\end{listingfloat}

\subsection{Using server-side state}

While the Hello Server example illustrates how client-server communication is
handled, most web applications need to keep some server-side state as well.
At first glance, this would seem to be a trivial problem; can we not just
create our state holding elements in \lstinline!main! as usual?
Figure \ref{lst:bad-state} describes an attempt at a solution using this idea.

\begin{listingfloat}
\begin{code}
main = do
  visitorref <- newIORef 0
  runApp config $ do
    count <- export . liftIO $ do
      atomicModifyIORef visitorref (\v -> (v+1, v+1))

    runClient $ do
      visitors <- onServer count
      alert ("Your are visitor #" ++ show visitors)
\end{code}
\caption{Server-side state: a misguided first attempt}
\label{lst:bad-state}
\end{listingfloat}

While this may at first seem to solve the problem of server-side state, this
approach raises a number of questions. Does \lstinline!visitorref! live on the
client or the server, or perhaps on both? What happens if our state holding
element is initialized to a non-deterministic value, and what if we assign it
a value on the server and then attempt to read that value on the client?
Recall that, as explained in section \ref{sec:helloserver}, the function
passed to \lstinline!runApp! should for all intents and purposes be regarded as
the program's entry point.

To allow the use of server state while treating the final argument to
\lstinline!runApp! (from here on always referred to as \lstinline!appMain!)
as the program's entry point, we may want to allow arbitrary IO computations to
be lifted into the \lstinline!App! monad:

\begin{code}
main = runApp config $ do
  visitorref <- liftIO $ newIORef 0
  ...
\end{code}

However, this does not answer the question of where \lstinline!visitorref!
lives or where any side effects caused by its creation will take place.
To solve this problem once and for all, we need to introduce a way to lift
arbitrary IO computations, but ensure that said computations are executed on
the server and nowhere else. This is accomplished using a more restricted
version of \lstinline!liftIO!:

\begin{code}
liftServerIO :: IO a -> App (Useless a)
\end{code}

In contrast to \lstinline!liftIO!, the result value of \lstinline!liftServerIO!
is wrapped in the \lstinline!Useless! type. As the name implies, this type is
completely opaque and thus useless unless you can somehow extract its contained
value, something which is only possible using a single function:

\begin{code}
mkUseful :: Useless a -> Server a
\end{code}

Therein lies the secret to restricting \lstinline!liftServerIO! to the
server: the only function capable of turning a \lstinline!Useless! value into
a useful one lives in the server monad. Any client side code is thus free to
completely ignore executing computations lifted using \lstinline!liftServerIO!;
since the result of a server lifted computation is never observable on the
client, the client has no obligation to even produce such a value.
Figure \ref{lst:good-state} shows how to make proper use of server-side state.

\begin{listingfloat}
\begin{code}
main =
  runApp config $ do
    uselessref <- liftServerIO $ newIORef 0

    count <- export $ do
      r <- mkUseful uselessref
      liftIO $ atomicModifyIORef r (\v -> (v+1, v+1))

    runClient $ do
      visitors <- onServer count
      alert ("Your are visitor #" ++ show visitors)
\end{code}
\caption{Server-side state: doing it properly}
\label{lst:good-state}
\end{listingfloat}

\subsection{The chatbox, revisited}

Now that we have seen how to implement both network communication, we are ready
to revisit the chatbox program, this time using our improved programming model.
Since we are now writing the entire application, both client and server, as
opposed to the client part from our motivating example, our program has three
new responsibilities.

\begin{itemize}
  \item We need to add connecting users to a list of message recipients;
  \item users leaving the site need to be removed from the recipient list; and
  \item chat messages need to be distributed to all users in the list.
\end{itemize}

With this in mind, we begin by importing a few modules we are going to need and
define the type for our recipient list.

\begin{code}
import Data.List (find)
import Haste.App
import Haste.App.Concurrent
import qualified Control.Concurrent as CC

type Recipient = (SessionID, CC.MVar String)
type RcptList = CC.MVar [Recipient]
\end{code}

We use an \lstinline!MVar! from \lstinline!Control.Concurrent! to store the
list of recipients. A recipient will be represented by a \lstinline!SessionID!,
an ID used by Haste.App to identify user sessions, and an \lstinline!MVar! into
which new chat messages sent to the recipient will be written. Next, we define
our handshake RPC function.

\begin{code}
srvHello :: Useless RcptList -> Server ()
srvHello uRcpts = do
  recipients <- mkUseful uRcpts
  sid <- getSessionID
  liftIO $ do
    rcptMVar <- CC.newEmptyMVar
    CC.modifyMVar recipients $ \cs ->
      return ((sid, rcptMVar):cs ,())
\end{code}

The handshake is a simple operation; the connecting client just gets an
\lstinline!MVar! associated with its session identifier, and the two are then
prepended to the recipient list. Notice how the application's server state is
passed in as the function's first \lstinline!Useless RcptList! argument.

\begin{code}
srvSend :: Useless RcptList -> String -> Server ()
srvSend uRcpts message = do
    rcpts <- mkUseful uRcpts
    liftIO $ do
      recipients <- CC.withMVar rcpts return
      mapM_ (CC.forkIO . deliver message) recipients
  where
    deliver msg (_, rcptMVar) = CC.putMVar rcptMVar msg
\end{code}

The send function is slightly more complex. The incoming message is written to
the \lstinline!MVar! corresponding to each active session. Note the use of
\lstinline!forkIO!. Some recipients may be slow to empty their \lstinline!MVar!
to make room for a new message. We don't want to force all recipients to wait
for the slowest one, so we spawn a new lightweight thread for each recipient.

\begin{code}
srvAwait :: Useless RcptList -> Server String
srvAwait uRcpts = do
  rcpts <- mkUseful uRcpts
  sid <- getSessionID
  liftIO $ do
    recipients <- CC.withMVar rcpts return
    case find ((== sid) . fst) recipients of
      Just (_, mv) -> CC.takeMVar mv
      _            -> fail "Unregistered session!"
\end{code}

The final server operation, notifying users of pending messages, finds the
appropriate \lstinline!MVar! to wait on by searching the recipient list for the
session identifier of the calling user, and then blocks until a message arrives
in said \lstinline!MVar!. This is a little different from the other two
operations, which perform their work as quickly as possible and then return
immediately.

If the caller's session identifier could not be found in the
recipient list, it has for some reason not completed its handshake with the
server. If this is the case, we simply drop the session by throwing an error;
the session will be automatically cleaned up on the server side and an exception
is thrown to the client.

Having implemented our three server operations, all that's left is to tie them
to the client. We start by creating an \lstinline!MVar! to hold the server
state and exporting our three server operations.

\begin{code}
appMain :: App Done
appMain = do
  recipients <- liftServerIO $ CC.newMVar []

  hello <- export $ srvHello recipients
  awaitMsg <- export $ srvAwait recipients
  sendMsg <- export $ srvSend recipients

  runClient $ do
    withElems ["log","message"] $ \[log,msgbox] -> do
      onServer hello
\end{code}

Notice that the \lstinline!recipients! list is passed to our three server
operations \emph{before} they are exported; since \lstinline!recipients! is
created on the server and inaccessible to the client side, it is not possible
to pass it over the network as an RPC argument. Even if it were possible,
passing server-private state back and forth over the network would be highly
undesirable.

The \lstinline!withElems! function is part of Haste's DOM manipulation library;
it locates references to the DOM nodes with the given identifiers, in this case
the variable \lstinline!log! is bound to the node with the identifier ``log'',
and \lstinline!inp! is bound to the node identifier by ``msg''. These are the
same DOM nodes that were referenced in our original example, and refer
to the chat log window and the text input field respectively. After locating
all the needed UI elements, the client proceeds to register itself with the
server's recipient list using the \lstinline!hello! export.

\begin{code}
      let recvLoop chatlines = do
            setProp log "value" $ unlines chatlines
            message <- onServer awaitMsg
            recvLoop (message : chatlines)
      fork $ recvLoop []
\end{code}

The \lstinline!recvLoop! function perpetually asks the server for new messages
and updates the chat log whenever one arrives. Note that unlike the
\lstinline!onmessage! callback of the Javascript version of this example,
\lstinline!recvLoop! is acting as a completely self-contained process with
linear program flow, keeping track of its own state and only reaching out to
the outside world to write its state to the chat log whenever necessary. As
the \lstinline!awaitMsg! function blocks until a message arrives,
\lstinline!recvLoop! will make exactly one iteration per received message.

\begin{code}
      msgbox `onEvent` OnKeyPress $ \13 -> do
        msg <- getProp msgbox "value"
        setProp msgbox "value" ""
        onServer (sendMsg <.> msg)
\end{code}

This is the final part of our program; we set up an event handler to clear the
input box and send its contents off to the server whenever the user hits return
(character code 13) while the input box has focus. Finally, all that is left
is to actually launch our application.

\begin{code}
main :: IO ()
main =
  runApp (mkConfig "ws://localhost:1111" 1111) appMain
\end{code}

Using this approach, creating web applications with strongly typed
communication becomes easy. The discerning reader may be slightly annoyed at
the need to extract the contents from \lstinline!Useless! values at each point
of use. Indeed, in a simple example such as this, the source clutter caused by
this becomes a disproportionate irritant. Fortunately, most web applications
tend to have more complex client-server interactions, reducing this overhead
significantly.

\section{Implementation}

\begin{itemize}
  \item Poor man's concurrency monad
  \item Conditional compilation
  \item Printf trick to turn export into [JSON] -> f
  \item JSON over WebSockets used for transport
  \item Server event loop awaits commands, dispatches functions, sends data back
  \item Client sends command with nonce and blocks on MVar, event loop fills
        MVar when return value with proper nonce arrives
  \item Throw away unused stuff on client
\end{itemize}

\section{The Haste compiler}

In order to allow the same language to be used on both client and server, we
obviously need some way to compile that language into Javascript. To this end,
we make use of the Haste compiler\ \cite{haste}, developed as a MSc thesis and
extended as part of this work. Haste builds on the GHC compiler to provide
the full Haskell language, including most GHC-specific extensions, in the
browser.

\subsection{Choosing a compiler}

Haste is by no means the only Javascript-targeting compiler for a purely
functional language. In particular, the GHC-based GHCJS\ \cite{ghcjs} and
UHC\ \cite{uhc} compilers are both capable of compiling standard Haskell into
Javascript; the Fay\ \cite{fay} language was designed from the ground up to
target the web space using a subset of Haskell; and there exist solutions for
compiling Erlang\ \cite{jserlang} and Clean\ \cite{jsclean} to Javascript as
well.

In the face of this multitude of options, why did we settle on Haste? We make
the following demands of a candidate source language:

\begin{itemize}
  \item The language must provide a static type system, since one of our
        primary concerns is to reduce defect rates through static typing of
        the client-server communication channel.
  \item The language must be compilable to both Javascript and a format
        suitable for server-side execution as we want our web applications
        to be written as a single, monolithic program.
  \item We also want our language to be practical, in the sense that its output
        should not be disproportionately large or slow. This is more important
        for the client part, where we have to contend with slow connections and
        the overhead of running within an interpreter, than for the server part.
\end{itemize}

Our first criterion rules out the dynamically typed Erlang. The second rules
out Fay which unfortunately lacks critical Haskell features, most notably type
classes.

This still leaves us with four options: Clean, GHCJS, Haste and UHC. However,
the output from GHCJS is prohibitively large - often in the range of several
megabytes for very simple programs - which rules it out for our use case, in
spite of providing slightly better compatibility with low-level GHC features
than Haste. Similarly, the code produced by the UHC Javascript backend tends to
be relatively slow due to it lacking the more mature optimization capabilities
of GHC, as well as being in the same ballpark as GHCJS for size.\ \cite{haste}

This leaves us choosing between Clean and Haste for our client compiler. Since
most of our abstractions, both the cooperative multitasking and the type-safe
client-server communication itself, are easily expressed as monads, well
supported in Haskell thanks to do-notation, the choice fell to Haste.

\subsection{Design of the Haste compiler}

Haste is first and foremost intended to be a practical compiler; the ability
to produce code which is lean and fast enough to be used in the real world is
critical. To this end, we make several design choices and tradeoffs, some of
which would be viewed as detrimental in a more purist context.

One such tradeoff is the decision to make use of the facilities provided by the
web browser - most prominently the Javascript engine's native garbage
collector - as far as possible rather than implementing parts of the GHC
runtime in Javascript. In the same vein, a conscious effort is made to generate
as ``regular-looking'' code as possible. In some rare cases, this means having
to sacrifice compatibility with low-level GHC features. Most notably, Haste
does not support weak references or pre-emptive multitasking, as these would
require modifications to the runtime system and generated code which confound
the structure of the resulting Javascript and have a detrimental impact on code
size and execution speed.

GHCJS takes the opposite view, that faithful emulation of the GHC runtime is
paramount. Of course, this brings an impressive degree of compatibility, at the
cost of the blowup in code size mentioned previously.

\subsection{Implementation overview}

As previously mentioned, Haste offloads much of the heavy lifting of
compilation - parsing, type checking, intermediate code generation and many
optimizations - onto GHC, and takes over code generation after the STG
generation step, at the very end of the compilation process. STG\ \cite{stg} is
the last intermediate representation used by GHC before the final code
generation takes place and has several benefits for use as Haste's target
language:

\begin{itemize}
  \item STG is still a functional intermediate representation, based on the
        lambda calculus. When generating code for a high level target language
        such as Javascript, where functions are first class objects, this
        allows for a higher level translation than when doing traditional
        compilation to lower level targets like stack machines or register
        machines. This in turn allows us to make more efficient use of the
        target language's runtime, leading to smaller, faster code.
  \item In contrast to Haskell itself and GHC's intermediate Core language, STG
        represents `thunks`, the construct used by GHC to implement non-strict
        evaluation, as closures which are explicitly created and evaluated.
        Closures are decorated with a wealth of information, such as their set
        of captured varibles, any type information needed for code generation,
        and so on. While extracting this information manually is not very hard,
        having this done for us means we can get away with a simpler
        compilation pipeline.
  \item The language is very small, essentially only comprising lambda
        abstraction and application, plus primitive operations and facilities
        for calling out to other languages. Again, this allows the Haste
        compiler to be a very simple thing indeed.
  \item Any extensions to the Haskell language implemented by GHC will already
        have been translated into this very simple intermediate format,
        allowing us to support basically any extension GHC supports without
        effort.
  \item Application of external functions is always saturated, as is
        application of most other functions. This allows for compiling most
        function applications into simple Javascript function calls, limiting
        the use of the slower dynamic techniques required to handle curried
        functions in the general case\ \cite{fastcurry} to cases where it is
        simply not possible to statically determine the arity of a function.
\end{itemize}

In light of its heavy reliance on STG, it may be more correct to categorize
Haste as an STG compiler rather than a Haskell compiler.

\subsection{Data representation}

The runtime data representation of Haste programs is kept as close to regular
Javascript programs as possible. The numeric types are represented using the
Javascript \lstinline!Number! type, which is defined as the IEEE754 double
precision floating point type. This obviously adds some overhead to operations
on integers as overflow and non-integer divisions must be handled. However, this
is common practice in hand-written Javascript as well, and is generally handled
efficiently by Javascript engines.

Values of non-primitive data types in Haskell consist of a data constructor and
zero or more arguments. In Haste, these values are represented using arrays,
with the first element representing the data constructor and the following
values representing its arguments. For instance, the value \lstinline!42 :: Int!
is represented as \lstinline![0, 42]!, the leading \lstinline!0! representing
the zeroth constructor of the \lstinline!Int! type and the \lstinline!42!,
obviously, representing the ``machine'' integer. It may seem strange that a
limited precision integer is represented using one level of indirection rather
than as a simple number, but recall that the \lstinline!Int! type is defined by
GHC as \lstinline!data Int = I# Int#!.

Functions are represented as plain Javascript functions, one of the blessings
of targeting a high level language, and application is can therefore be
implemented as its Javascript counterpart in most cases. In the general case,
however, functions may be curried. For such cases where the arity of an applied
function can not be determined statically, application is implemented using the
eval/apply method described in \cite{fastcurry} instead.

\subsection{Interfacing with Javascript}

While Haste supports the Foreign Function Interface inherited from GHC, with
its usual features and limitations \ \cite{ffi}, it is often impractical to
work within the confines of an interface designed for communication on a very
low level. For this reason Haste sports its own method for interacting with
Javascript as well, which allows the programmer to pass any value back and
forth between Haskell and Javascript, as long as she can come up with a way to
translate this value between its Haskell and Javascript representations.
Of course, not performing any translation at all is also a valid
``translation'', which allows Haskell code to store any Javascript value for
later retrieval without inspecting it and vice versa. The example given in
figure \ref{lst:ffi} implements mutable variables using this custom Javascript
interface.

\begin{listingfloat}
\begin{code}
import Haste.Foreign

-- A MutableVar is completely opaque to Haskell code
-- and is only ever manipulated in Javascript. Thus,
-- we use the Unpacked type to represent it,
-- indicating a completely opaque value.
newtype MutableVar a = MV Unpacked

instance Marshal (MutableVar a) where
  pack          = MV
  unpack (MV x) = x

newMutable :: Marshal a => a -> IO (MutableVar a)
newMutable = ffi "(function(x) {return {val: x};})"

setMutable :: Marshal a => MutableVar a -> a -> IO ()
setMutable = ffi "(function(m, x) {m.val = x;})"

getMutable :: Marshal a => MutableVar a -> IO a
getMutable = ffi "(function(m) {return m.val;})"
\end{code}
\caption{Mutable variables with \lstinline!Haste.Foreign!}
\label{lst:ffi}
\end{listingfloat}

The core of this interface consists of the \lstinline!ffi! function, which
allows the programmer to create a Haskell function from arbitrary Javascript
code. This function exploits Javascript's ability to parse and execute
arbitrary strings at run-time using the \lstinline!eval! function, coupled with
the fact that functions in Haste and in Javascript share the same
representation, to dynamically create a function object at runtime.
The \lstinline!ffi! function is typed using a well-known type class trick for
creating statically typed variadic functions\ \cite{printf}, and works very
much like the \lstinline!printf! function of Haskell's standard library.
When applied to one or more arguments instantiating the \lstinline!Marshal!
type class, the \lstinline!pack! function is applied to each argument,
marshalling them into their respective Javascript representations, before they
are passed to the dynamically created function. When that function returns,
the inverse \lstinline!unpack! function is applied to its return value before
it is passed back into Haskell land.

As the marshalling functions chosen for each argument and the foreign
function's return value depends on its type, the programmer must explicitly
specify the type of each function imported using \lstinline!ffi!; in this,
Haste's custom method is no different from the conventional FFI.

There are several benefits to this method, the most obvious being that new
marshallable types can be added by simply instantiating a type class. Thanks
to the lazy evaluation employed by Haste, each foreign function object is only
created once and then cached; any further calls to the same (Haskell) function
will reuse the cached function object. Implementation-wise, this method is also
very non-intrusive, requiring only the use of the normal FFI to import
Javascript's \lstinline!eval! function; no modification of the compiler is
needed.

\section{Discussion and related work}

\subsection{Related work}

Other approaches to this problem exist, notably in the form of the
Conductance\ \cite{conductance}, Duetto\ \cite{duetto} and
Sunroof\ \cite{sunroof} projects, all of which implement some synchronous
abstraction over the network between the client and the server, much like our
solution.

\paragraph{Conductance} Conductance is an application server built on
StratifiedJS, a Javascript language extension which adds a few niceties such as
cooperative multitasking and more concise syntax for many common tasks.
Conductance uses an RPC-based model for client-server communication, much like
our own, but also adds the possibility for the server to independently transmit
data back to the client through the use of shared variables or call back into
the client by way of function objects received via RPC call, as well as the
possibility for both client and server to seamlessly modify variables located
on the opposite end of the network.

While Conductance gets rid of the callback-based programming model endemic to
regular Javascript, it still suffers from many of its usual drawbacks. In
particular, the weak typing of Javascript poses a problem in that the
programmer is in no way reprimanded by her tools for using server APIs
incorrectly or trying to transmit values which can not be sensibly serialized
and de-serialized, such as DOM nodes. Wrongly typed programs will thus crash, or
even worse, gleefully keep running with erroneous state due to implicit type
conversions, rather than give the programmer some advance warning that something
is amiss.

We are also not completely convinced that the ability to implicitly pass data
back and forth over the network is a unilaterally good thing; while this indeed
provides the programmer some extra convenience, it also requires the programmer
to exercise extra caution to avoid inadvertently sending large amounts of data
over the network.

\paragraph{Duetto} Duetto is a C++ compiler targeting the web, written from
the ground up to produce code for both client and server simultaneously. It
utilizes the new attributes mechanism introduced in C++11 to designate
functions and data to live on either client or server side. Any calls to a
function on the other side of the network and attempts to access remote data
are implicit, requiring no extra annotations or scaffolding at the call site.

Much like Conductance, Duetto suffers somewhat from its heritage: while the
client-side code is not memory-unsafe, as it is not possible to generate
memory-unsafe Javascript code, its server-side counterpart unfortunately is.
Our reservations expressed about how network communication in Duetto can be
initiated implicitly apply to Duetto as well.

\paragraph{Sunroof} In contrast to the Conductance and Duetto, Sunroof is an
embedded language. Implemented as a Haskell library, it allows the programmer
to use Haskell to write code which is compiled to Javascript and executed on
the client. The language can best be described as having Javascript semantics
with Haskell's type system. Communication between client and server is
accomplished through the use of ``downlinks'' and ``uplinks'', allowing for
data to be sent to and from the client respectively.

Sunroof is completely type-safe, in the DSL itself as well as in the
communication with the Haskell host. However, the fact that client and server
must be written in two separate languages - any code used to generate
Javascript must of course be built solely from the primitives of the Sunroof
language, precluding use of general Haskell code - makes code reuse hard.

\paragraph{Advantages to our approach} We believe that our approach has a
number of distinct advantages to the aforementioned attacks on the problem.

Our approach gives the programmer access to the same strongly typed,
general-purpose functional language on both client and server; any code which
may be of use to both client and server is effortlessly shared, leading to less
duplication of code and increased possibilities for reusing third party
libraries.

Any and all communication between client and server is both strongly typed
and made explicit by the use of the \lstinline!onServer! function, with the
programmer having complete control over the serialization and de-serialization
of data using the appropriate type classes. Aside from the obvious advantages
of type safety, making the crossing of the network boundary explicit aids the
programmer in making an informed decision as to when and where server
communication is appropriate, as well as helps prevents accidental transmission
of sensitive information intended to stay on the client.

Our programming model is implemented as a library, assuming only two Haskell
compilers, one targeting Javascript and one targeting the programmer's server
platform of choice. While we use Haste as our Javascript-targeting compiler,
modifying our implementation to use GHCJS or even the Javascript backend of UHC
would be entirely straightforward. This implementation not only allows for
greater flexibility, but also eliminates any potentially hairy interaction with
compiler internals.

\subsection{Limitations}
\label{sec:limitations}

\paragraph{Client-centricity} Unlike any of the related works, our approach
takes a firm stand, regarding the client as the driver in the client-server
relationship with the server taking on the role of a passive computational
or storage resource. The server may thus not call back into the client at
arbitrary points but is instead limited to returning answers to client-side
queries. This is clearly less flexible than the back-and-forth model of Sunroof
and Duetto or the shared variables of Conductance. However, we believe that
this restriction makes program flow easier to follow and comprehend. Like the
immutability of Haskell, this model gives programmers a not-so-subtle hint
as to how they may want to structure their programs. Should this model prove
insufficiently expressive, extending our existing model with an
\lstinline!onClient! counterpart to \lstinline!onServer! would be relatively
straightforward.

\paragraph{Preventing divergence} While our solution completely safe from the
invocation point of the \lstinline!runApp! function onward, capturing
IO-dependent variables introduced before this invocation may lead to the client
and server execution paths diverging. Recall that our solution does not modify
either of the compilers used. As such, both will produce code to execute all
computations called by \lstinline!main!. Consider the following example:

\begin{code}
main = do
  captured <- randomRIO (False, True)

  runApp config $ do
    diverged <- export $ \transmitted -> do
      if captured == transmitted
        then return ``All OK''
        else return ``Diverged''

    runClient $ do
      divergence <- onServer (diverged <.> captured)
      alert divergence
\end{code}

The dialog box displayed by this program will read ``All OK'' roughly half
of the times it is run, while in half of the runs it will read ``Diverged''.
The reason for this is that the \lstinline!randomRIO! computation will be
executed twice - once on the client, and once on the server. Whenever the
computation returns differening results, the client and the server will have
conflicting opinions on the actual value of \lstinline!captured!, causing any
code which - quite reasonably - assumes that variables are consistent between
client and server to fail.

This is easily remedied however, by a slight compiler modification. Instead of
using \lstinline!main :: IO ()! as the program entry point, introduce a function
\lstinline!appMain :: App Done! for this purpose, and let the runtime system
start the application by invoking \lstinline!runApp config appMain!, where
\lstinline!config! contains the URL and port configuration for the server, given
by the programmer using a compiler flag. As this modification only affects the
program's entry point, which is already given ``special'' treatment in Haskell
compilers, and may even be implemented using the plugin mechanism of GHC,
requiring no modification of the compiler code at all, we believe this to be a
practical, non-intrusive way to remove this limitation, and are planning on
including this modification in our implementation.

The astute reader may notice that the same applies to code using
\lstinline!unsafePerformIO! to declare IO-dependent top-level values, and that
this problem is unaffected by changing the type of the program's entry point.
While this is indeed true, we feel that since it is quite possible to break
any Haskell program using \lstinline!unsafePerformIO!, even subverting the type
system, this is not a problem with our solution.

\paragraph{Environment consistency} As our programming model uses two different
compilers to generate client and server code, it is crucial to keep the package
environments of the two in sync. A situation where, for instance, a module is
visible to one compiler but not to the other will render many programs
uncompilable until this inconsistency is fixed.

This kind of divergence can be worked around using conditional compilation, but
is highly problematic even so; using a unified package database between the two
compilers, while problematic due to the differing natures of native and
Javascript compilation respectively, would be a significant improvement in this
area.

\section{Future work}

\paragraph{Information flow control} Web applications often make use of a wide
range of third party code for user tracking, advertising, collecition of
statistics and a wide range of other tasks. Any piece of code executing in the
context of a particular web session may not only interact with any other piece
of code executing in the same context, but may also perform basically limitless
communication with third parties and may thus, inadvertently or not, leak
information about the application state. This is obviously highly undesirable
for many applications, which is why there is ongoing work in controlling the
information flow within web applications\ \cite{jsflow}.

While this would indeed provide an effective defence towards attackers and
programming mistakes alike, there is value in being able to tell the two apart,
as well as in catching policy violations resulting from programming mistakes
as early as possible. An interesting venue of research would be to investigate
whether we can take advantage of our strong typing to generate security policies
for such an information flow control scheme, as well as ensure that this policy
is not violated at compile time. This could shorten development cycles as well
as give a reasonable level of confidence that any run time policy violation is
indeed an attempted attack.

\paragraph{Reactive client-server GUI toolkit} While we believe that our work
represents a step forward in making web applications more robust and easier to
develop, the basic model for updating the application's user-visible state is
still essentially imperative. Implementing an FRP-style\ \cite{ppfrp} GUI
programming model on top of our client-server abstraction may prove a fruitful
way to further improve the programmability of web-based distributed
applications.

\section{Conclusion}

We have presented a programming model which improves on the current state of
the art in client-server web application development. In particular, our
solution combines type safe communication between the client and the server
with purely functional semantics, clear demarcations as to when data is
transmitted and where a particular piece of code is executed, and the ability to
effortlessly share code between the client and the server.

Our model is based on a simple blocking concurrency model rather than explicit
continuations which is well suited for use with a GUI programming style based on
communicating processes with local state, and requires essentially no
modification of existing tools or compilers.

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Bracker, Gill (2014)]{sunroof}
J. Bracker and A. Gill: Sunroof: A monadic DSL for generating JavaScript, Practical Aspects of Declarative Languages, 65-80, 2014

\bibitem[Conductance application server (2014)]{conductance}
The Conductance application server, http://conductance.io

\bibitem[Dijkstra et al (2013)]{uhc}
A. Dijkstra, J. Stutterheim, A. Vermeulen, S. D. Swierstra: Building JavaScript applications with Haskell, Implementation and Application of Functional Languages, pp 37-52, 2013

\bibitem[Domoszlai et al (2011)]{jsclean}
L. Domoszlai, E. Brul, J. M. Jansen: Implementing a non-strict purely functional language in JavaScript, Acta Universitatis Sapientiae, 2011

\bibitem[Done (2012)]{fay}
C. Done: Fay, JavaScript, etc., http://chrisdone.com/posts/fay, 2012

\bibitem[Ekblad (2012)]{haste}
A. Ekblad: Towards a declarative web, MSc thesis, University of Gothenburg, 2012

\bibitem[Elliott (2009)]{ppfrp}
C. M. Elliott: Push-pull functional reactive programming, in Proceedings of the 2nd ACM SIGPLAN symposium on Haskell, 2009

\bibitem[Guthrie (2014)]{jserlang}
G. Guthrie: http://luvv.ie/2014/01/21/your-transpiler-to-javascript-toolbox/, 2014

\bibitem[Hedin et al (2014)]{jsflow}
D. Hedin, A. Birgisson, L. Bello and A. Sabelfeld: JSFlow: Tracking Information Flow in JavaScript and its APIs, 2014

\bibitem[Marlow, Peyton Jones (2004)]{fastcurry}
S. Marlow, S. Peyton Jones: Making a fast curry: push/enter vs. eval/apply for higher order languages, 2004

\bibitem[Nazarov (2012)]{ghcjs}
V. Nazarov: GHCJS Haskell to Javascript Compiler, https://github.com/ghcjs/ghcjs

\bibitem[Peyton Jones (1992)]{stg}
S. Peyton Jones: Implementing lazy functional languages on stock hardware: the Spineless Tagless G-machine, Journal of Functional Programming, 1992

\bibitem[Peyton Jones (2001)]{ffi}
S. Peyton Jones: Tackling the awkward squad: monadic input/output, concurrency, exceptions, and foreign-language calls in Haskell

\bibitem[Pignotti (2013)]{duetto}
A. Pignotti: Duetto: a C++ compiler for the Web going beyond emscripten and node.js, http://leaningtech.com/duetto/blog/2013/10/31/Duetto-Released/, 2013

\bibitem[Taylor (2013)]{printf}
C. Taylor: Polyvariadic Functions and Printf, http://chris-taylor.github.io/blog/2013/03/01/how-haskell-printf-works/, 2013

\end{thebibliography}


\end{document}
